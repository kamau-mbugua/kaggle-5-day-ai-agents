"""
Example 3: Running Evaluations

Demonstrates:
- Using the adk eval CLI command
- Understanding evaluation output
- Interpreting results files
- Common evaluation scenarios

Key Pattern:
adk eval <agent_dir> <evalset_file> --config_file_path=<config> --print_detailed_results

Run:
python examples/example_3_running_evaluations.py
"""

import asyncio
import sys
import os
import json
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import load_api_key


async def demo_running_evaluations():
    """
    Demonstrate running evaluations.

    Shows:
    1. The adk eval CLI command
    2. Understanding command options
    3. Reading evaluation output
    4. Interpreting results files
    5. Common evaluation scenarios
    """
    print("\n" + "=" * 80)
    print("Example 3: Running Evaluations")
    print("=" * 80)

    # Demo 1: The adk eval command
    print("\n" + "-" * 80)
    print("DEMO 1: The adk eval CLI Command")
    print("-" * 80)
    print()
    print("Basic Syntax:")
    print("  adk eval <agent_directory> <evalset_file>")
    print()
    print("Example:")
    print("  adk eval home_automation_agent \\")
    print("           home_automation_agent/integration.evalset.json")
    print()
    print("What happens:")
    print("  1. ADK loads the agent from agent_directory")
    print("  2. ADK reads test cases from evalset_file")
    print("  3. Agent processes each test case")
    print("  4. ADK compares actual vs expected")
    print("  5. Scores are calculated")
    print("  6. Results are saved and displayed")
    print()

    # Demo 2: Command options
    print("-" * 80)
    print("DEMO 2: Command Options")
    print("-" * 80)
    print()
    print("Available Options:")
    print()
    print("1. --config_file_path")
    print("   Purpose: Specify test configuration with thresholds")
    print("   Usage:   --config_file_path=path/to/test_config.json")
    print("   Default: If not specified, uses ADK defaults")
    print()
    print("2. --print_detailed_results")
    print("   Purpose: Show detailed output for each test case")
    print("   Usage:   --print_detailed_results")
    print("   Default: Shows summary only")
    print()
    print("3. --model")
    print("   Purpose: Override the model used for evaluation")
    print("   Usage:   --model=gemini-2.0-flash-exp")
    print("   Default: Uses agent's configured model")
    print()
    print("Full Example:")
    print("  adk eval home_automation_agent \\")
    print("           home_automation_agent/integration.evalset.json \\")
    print("           --config_file_path=home_automation_agent/test_config.json \\")
    print("           --print_detailed_results")
    print()

    # Demo 3: Understanding output
    print("-" * 80)
    print("DEMO 3: Understanding Evaluation Output")
    print("-" * 80)
    print()
    print("Console Output Structure:")
    print()
    print("1. SUMMARY (always shown)")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Evaluation Summary                 â”‚")
    print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("   â”‚ Total Test Cases: 5                â”‚")
    print("   â”‚ Passed: 3                          â”‚")
    print("   â”‚ Failed: 2                          â”‚")
    print("   â”‚ Pass Rate: 60.0%                   â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("2. DETAILED RESULTS (with --print_detailed_results)")
    print("   For each test case:")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Test: test_turn_on_lights          â”‚")
    print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("   â”‚ Status: âœ… PASSED                  â”‚")
    print("   â”‚                                    â”‚")
    print("   â”‚ Metrics:                           â”‚")
    print("   â”‚   response_match_score: 0.92       â”‚")
    print("   â”‚   tool_trajectory_avg: 1.0         â”‚")
    print("   â”‚                                    â”‚")
    print("   â”‚ Expected Response:                 â”‚")
    print("   â”‚   'The living room lights are on.' â”‚")
    print("   â”‚                                    â”‚")
    print("   â”‚ Actual Response:                   â”‚")
    print("   â”‚   'Living room lights turned on.'  â”‚")
    print("   â”‚                                    â”‚")
    print("   â”‚ Expected Tools:                    â”‚")
    print("   â”‚   set_device_status(...)           â”‚")
    print("   â”‚                                    â”‚")
    print("   â”‚ Actual Tools:                      â”‚")
    print("   â”‚   set_device_status(...)           â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()

    # Demo 4: Results file format
    print("-" * 80)
    print("DEMO 4: Results File Format")
    print("-" * 80)
    print()
    print("After evaluation, ADK creates a results file:")
    print("  <evalset_file>.results")
    print()
    print("Example: integration.evalset.json.results")
    print()
    print("Structure:")
    print("{")
    print('  "eval_set_id": "home_automation_integration_suite",')
    print('  "eval_cases": [')
    print("    {")
    print('      "eval_id": "test_turn_on_lights",')
    print('      "pass": true,')
    print('      "conversation": [')
    print("        {")
    print('          "user_content": {...},')
    print('          "final_response": {...},')
    print('          "intermediate_data": {...},')
    print('          "metrics": {')
    print('            "response_match_score": 0.92,')
    print('            "tool_trajectory_avg_score": 1.0')
    print("          }")
    print("        }")
    print("      ]")
    print("    }")
    print("  ]")
    print("}")
    print()
    print("Key Fields:")
    print("  â€¢ pass: Boolean (true/false) based on thresholds")
    print("  â€¢ metrics: Calculated scores for this test case")
    print("  â€¢ final_response: Agent's actual response")
    print("  â€¢ intermediate_data: Agent's actual tool calls")
    print()

    # Demo 5: Score interpretation
    print("-" * 80)
    print("DEMO 5: Interpreting Scores")
    print("-" * 80)
    print()
    print("response_match_score:")
    print("  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()
    print("  0.0 - 0.3  âŒ Completely different")
    print("  0.3 - 0.6  âš ï¸  Partially related")
    print("  0.6 - 0.8  ğŸŸ¡ Similar meaning")
    print("  0.8 - 1.0  âœ… Very similar")
    print()
    print("  Example Scores:")
    print()
    print("  Expected: 'The living room lights are now on.'")
    print()
    print("  Actual: 'Living room lights turned on.'")
    print("  Score: 0.92 âœ… (semantically equivalent)")
    print()
    print("  Actual: 'I turned on the lights.'")
    print("  Score: 0.75 ğŸŸ¡ (similar but missing location)")
    print()
    print("  Actual: 'What would you like me to do?'")
    print("  Score: 0.15 âŒ (completely different)")
    print()
    print("tool_trajectory_avg_score:")
    print("  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()
    print("  1.0  âœ… Perfect match (tool + all args correct)")
    print("  0.7  âš ï¸  Partial match (some args wrong)")
    print("  0.0  âŒ No match (wrong tool or not called)")
    print()
    print("  Example Scores:")
    print()
    print("  Expected: set_device_status(location='living_room', device='lights', status='on')")
    print()
    print("  Actual: set_device_status(location='living_room', device='lights', status='on')")
    print("  Score: 1.0 âœ… (exact match)")
    print()
    print("  Actual: set_device_status(location='bedroom', device='lights', status='on')")
    print("  Score: 0.67 âš ï¸  (1 of 3 args wrong)")
    print()
    print("  Actual: No tool called")
    print("  Score: 0.0 âŒ (tool not used)")
    print()

    # Demo 6: Common scenarios
    print("-" * 80)
    print("DEMO 6: Common Evaluation Scenarios")
    print("-" * 80)
    print()
    print("Scenario 1: All Tests Pass âœ…")
    print("  Output:")
    print("    Total: 5, Passed: 5, Failed: 0")
    print("    Pass Rate: 100%")
    print()
    print("  Action:")
    print("    âœ… Agent is working correctly!")
    print("    âœ… Safe to deploy or merge changes")
    print()
    print("Scenario 2: Some Tests Fail âŒ")
    print("  Output:")
    print("    Total: 5, Passed: 3, Failed: 2")
    print("    Pass Rate: 60%")
    print()
    print("  Action:")
    print("    ğŸ” Review failed test details")
    print("    ğŸ› Identify root cause (response vs tool issue)")
    print("    ğŸ”§ Fix agent instructions or tool implementations")
    print("    ğŸ”„ Re-run evaluation")
    print()
    print("Scenario 3: Low Response Scores ğŸŸ¡")
    print("  Symptom:")
    print("    response_match_score: 0.45 (below 0.8 threshold)")
    print("    tool_trajectory_avg_score: 1.0")
    print()
    print("  Root Cause:")
    print("    â€¢ Agent using different phrasing")
    print("    â€¢ Response format changed")
    print("    â€¢ Missing expected information")
    print()
    print("  Action:")
    print("    ğŸ“ Update agent instructions for response format")
    print("    ğŸ¯ Add examples of expected responses")
    print("    âš–ï¸  Or adjust threshold if semantically correct")
    print()
    print("Scenario 4: Low Tool Scores âš ï¸")
    print("  Symptom:")
    print("    response_match_score: 0.95")
    print("    tool_trajectory_avg_score: 0.33 (below 1.0 threshold)")
    print()
    print("  Root Cause:")
    print("    â€¢ Wrong tool parameters")
    print("    â€¢ Tool not called at all")
    print("    â€¢ Wrong tool selected")
    print()
    print("  Action:")
    print("    ğŸ”§ Fix tool parameter extraction")
    print("    ğŸ“‹ Improve tool descriptions")
    print("    ğŸ“ Add examples to agent instructions")
    print()

    # Demo 7: Workflow
    print("-" * 80)
    print("DEMO 7: Evaluation Workflow")
    print("-" * 80)
    print()
    print("Complete Evaluation Cycle:")
    print()
    print("1ï¸âƒ£  Initial Run")
    print("   $ adk eval agent evalset.json --config_file_path=config.json")
    print("   Result: 3/5 passed (60%)")
    print()
    print("2ï¸âƒ£  Analyze Failures")
    print("   $ cat evalset.json.results | python analyze.py")
    print("   Finding: 2 tests failed due to wrong location parameter")
    print()
    print("3ï¸âƒ£  Fix Agent")
    print("   â€¢ Update agent instructions")
    print("   â€¢ Improve parameter extraction")
    print("   â€¢ Add examples")
    print()
    print("4ï¸âƒ£  Re-run Evaluation")
    print("   $ adk eval agent evalset.json --config_file_path=config.json")
    print("   Result: 5/5 passed (100%) âœ…")
    print()
    print("5ï¸âƒ£  Deploy with Confidence")
    print("   All tests pass â†’ Safe to deploy!")
    print()

    # Demo 8: Hands-on example
    print("-" * 80)
    print("DEMO 8: Try It Yourself!")
    print("-" * 80)
    print()
    print("After completing Example 4, try running:")
    print()
    print("Step 1: Create the home automation agent")
    print("  (We'll do this in Example 4)")
    print()
    print("Step 2: Run evaluation")
    print("  $ cd home_automation_agent")
    print("  $ adk eval . integration.evalset.json \\")
    print("      --config_file_path=test_config.json \\")
    print("      --print_detailed_results")
    print()
    print("Step 3: Review results")
    print("  â€¢ Check console output")
    print("  â€¢ Open integration.evalset.json.results")
    print("  â€¢ Identify failures")
    print()
    print("Step 4: Fix and iterate")
    print("  â€¢ Update agent.py")
    print("  â€¢ Re-run evaluation")
    print("  â€¢ Verify improvements")
    print()

    # Demo 9: Best practices
    print("-" * 80)
    print("DEMO 9: Evaluation Best Practices")
    print("-" * 80)
    print()
    print("âœ… DO:")
    print()
    print("  â€¢ Run evaluations before every deployment")
    print("  â€¢ Use --print_detailed_results for debugging")
    print("  â€¢ Keep evalsets in version control")
    print("  â€¢ Add tests when fixing bugs (regression tests)")
    print("  â€¢ Run evaluations in CI/CD pipelines")
    print("  â€¢ Document expected behavior in test cases")
    print()
    print("âŒ DON'T:")
    print()
    print("  â€¢ Skip evaluation to save time")
    print("  â€¢ Manually modify results files")
    print("  â€¢ Delete failing tests to improve scores")
    print("  â€¢ Use evaluation as only quality measure")
    print("  â€¢ Ignore low scores without investigation")
    print()

    # Summary
    print("=" * 80)
    print("âœ… Running Evaluations Complete!")
    print("=" * 80)
    print()
    print("Key Takeaways:")
    print()
    print("  âœ… Use adk eval CLI for batch testing")
    print("  âœ… --print_detailed_results shows per-test details")
    print("  âœ… Results files contain full evaluation data")
    print("  âœ… Iterate: Run â†’ Analyze â†’ Fix â†’ Re-run")
    print("  âœ… 100% pass rate = deploy with confidence")
    print()
    print("Quick Reference:")
    print()
    print("  Basic:     adk eval <agent_dir> <evalset_file>")
    print("  With config:  + --config_file_path=<config>")
    print("  Detailed:     + --print_detailed_results")
    print()
    print("Next Steps:")
    print("  â†’ Example 4: Analyze failures and fix agents")
    print("  â†’ Create your own home automation agent")
    print("  â†’ Run real evaluations!")
    print()


if __name__ == "__main__":
    asyncio.run(demo_running_evaluations())
